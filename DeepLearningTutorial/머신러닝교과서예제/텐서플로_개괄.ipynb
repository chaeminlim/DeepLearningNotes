{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서플로의 주요 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서: 테이터를 담고 있는 다차원 배열에 대한 일반화된 수학적 용어.\n",
    "텐서 차원을 일반적으로 랭크 라고 부른다.\n",
    "\n",
    "랭크 - 차원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서의 랭크와 크기를 확인하는 방법\n",
    "\n",
    "tf.rank 함수를 사용하여 텐서 랭크를 확인할 수 있다. 예를 들어 X가 텐서라면 X.get_shape()을 사용하여 크기를 구합니다. 이 메서드는 TensorShape이라는 특별한 클래스의 객체를 반환환다. tf.rank 함수와 텐서의 get_shape 메서드를 사용하여 텐서 객체의 랭크와 크기를 추출\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "t1 = tf.constant(np.pi)\n",
    "t2 = tf.constant([1, 2, 3, 4])\n",
    "t3 = tf.constant([[1, 2], [3, 4]])\n",
    "\n",
    "## 랭크를 구한다.\n",
    "r1 = tf.rank(t1)\n",
    "r2 = tf.rank(t2)\n",
    "r3 = tf.rank(t3)\n",
    "\n",
    "## 크기를 구한다.\n",
    "s1 = t1.get_shape()\n",
    "s2 = t2.get_shape()\n",
    "s3 = t3.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크기 () (4,) (2, 2)\n",
      "랭크 0 1 2\n"
     ]
    }
   ],
   "source": [
    "print('크기', s1, s2, s3)\n",
    "print('랭크', r1.numpy(), r2.numpy(), r3.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서를 다차원 배열로 변환\n",
    "\n",
    "일부 연산들은 넘파이 배열 연산과 매우 비슷하게 작동.\n",
    "랭크 2 이상인 텐서를 다룰 때 전치 같은 변환은 주의를 기울여야 한다.\n",
    "넘파이는 shape 속성으로 배열 크기를 얻는다.\n",
    "TF 는 tf.get_shape 함수를 사용한다.\n",
    "또는 텐서의 shape 속성을 사용한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.  2.  3.  3.5]\n",
      " [4.  5.  6.  6.5]\n",
      " [7.  8.  9.  9.5]], shape=(3, 4), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1., 2., 3., 3.5],\n",
    "              [4., 5., 6., 6.5],\n",
    "              [7., 8., 9., 9.5]])\n",
    "\n",
    "T1 = tf.constant(arr)\n",
    "print(T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = T1.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1의 크기 (3, 4)\n",
      "T1의 크기 (3, 4)\n"
     ]
    }
   ],
   "source": [
    "print('T1의 크기', s)\n",
    "print('T1의 크기', T1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2 = tf.Variable(np.random.normal(size = s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 4) dtype=float64, numpy=\n",
      "array([[ 0.14776593,  0.17795802, -2.04774627, -1.06025732],\n",
      "       [ 2.64173889,  0.0084694 ,  0.03339911, -0.16470154],\n",
      "       [-2.31474699,  0.21494321,  0.70216527,  0.77758085]])>\n"
     ]
    }
   ],
   "source": [
    "print(T2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3,) dtype=float64, numpy=array([0.51213483, 0.11894048, 0.38058718])>\n"
     ]
    }
   ],
   "source": [
    "T3 = tf.Variable(np.random.normal(size = s[0 ]))\n",
    "print(T3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[[1.  2.  3.  3.5 4.  5.  6.  6.5 7.  8.  9.  9.5]]], shape=(1, 1, 12), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "T4 = tf.reshape(T1, shape=[1,1,-1])\n",
    "print(T4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1.  2.  3.  3.5]\n",
      "  [4.  5.  6.  6.5]\n",
      "  [7.  8.  9.  9.5]]], shape=(1, 3, 4), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "T5 = tf.reshape(T1, shape =[1, 3, -1])\n",
    "print(T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1. ]\n",
      "  [4. ]\n",
      "  [7. ]]\n",
      "\n",
      " [[2. ]\n",
      "  [5. ]\n",
      "  [8. ]]\n",
      "\n",
      " [[3. ]\n",
      "  [6. ]\n",
      "  [9. ]]\n",
      "\n",
      " [[3.5]\n",
      "  [6.5]\n",
      "  [9.5]]], shape=(4, 3, 1), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "T6 = tf.transpose(T5, perm=[2, 1, 0])\n",
    "print(T6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1.  4.  7. ]\n",
      "  [2.  5.  8. ]\n",
      "  [3.  6.  9. ]\n",
      "  [3.5 6.5 9.5]]], shape=(1, 4, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "T7 = tf.transpose(T5, perm = [0, 2, 1])\n",
    "print(T7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=36, shape=(1, 3, 2), dtype=float64, numpy=\n",
      "array([[[1., 2.],\n",
      "        [4., 5.],\n",
      "        [7., 8.]]])>, <tf.Tensor: id=37, shape=(1, 3, 2), dtype=float64, numpy=\n",
      "array([[[3. , 3.5],\n",
      "        [6. , 6.5],\n",
      "        [9. , 9.5]]])>]\n"
     ]
    }
   ],
   "source": [
    "t5_split = tf.split(T5, num_or_size_splits = 2, axis = 2)\n",
    "print(t5_split) ## 출력은 텐서의 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], shape=(5, 1), dtype=float32) tf.Tensor(\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]], shape=(5, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "t1 = tf.ones(shape=(5, 1), dtype=tf.float32)\n",
    "t2 = tf.zeros(shape=(5, 1), dtype = tf.float32)\n",
    "print(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]], shape=(10, 1), dtype=float32) tf.Tensor(\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]], shape=(5, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "t3 = tf.concat([t1, t2], axis = 0)\n",
    "t4 = tf.concat([t1, t2], axis = 1)\n",
    "print(t3,t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow 계산 그래프 이해 1. x버전\n",
    "\n",
    "1. 비어있는 새로운 계산 그래프르를 만든다.\n",
    "2. 계산 그래프에 노드를 추가한다.\n",
    "3. 그래프를 실행한다.\n",
    "   <br> a.  새로운 세션을 시작한다.<br>\n",
    "   <br> b. 그래프에 있는 변수를 초기화한다. <br>\n",
    "   <br> c. 이 세션에서 계산 그래프를 실행한다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow 계산 그래프 2.x 버전\n",
    "\n",
    "텐서플로 2 버전에서는 tf.function 데코레이터를 사용하여 일반 파이썬 함수를 호출 가능한 그래프 객체로 만든다.\n",
    "마치 tf.Graph와 tf.Session 을 합쳐 놓은 것처럼 생각할 수 있다. 앞 코드를 tf.function 데코레이털르 사용하여 작성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def simple_func():\n",
    "    a = tf.constant(1)\n",
    "    b = tf.constant(2)\n",
    "    c = tf.constant(3)\n",
    "    \n",
    "    z = 2*(a - b) + c\n",
    "    return z\n",
    "\n",
    "print(simple_func().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.eager.def_function.Function'>\n"
     ]
    }
   ],
   "source": [
    "print(simple_func.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 같은 표현\n",
    "def simple_func():\n",
    "    a = tf.constant(1)\n",
    "    b = tf.constant(2)\n",
    "    c = tf.constant(3)\n",
    "    \n",
    "    z = 2*(a - b) + c\n",
    "    return z\n",
    "\n",
    "simple_func = tf.function(simple_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.function으로 감싼 함수 안의 연산은 자동으로 텐서플로 그래프에 포함되어 실행된다. 이를 AutoGraph 기능이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'Const' type=Const>,\n",
       " <tf.Operation 'Const_1' type=Const>,\n",
       " <tf.Operation 'Const_2' type=Const>,\n",
       " <tf.Operation 'sub' type=Sub>,\n",
       " <tf.Operation 'mul/x' type=Const>,\n",
       " <tf.Operation 'mul' type=Mul>,\n",
       " <tf.Operation 'add' type=AddV2>,\n",
       " <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_func = simple_func.get_concrete_function()\n",
    "con_func.graph.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "node {\n",
       "  name: \"Const\"\n",
       "  op: \"Const\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"value\"\n",
       "    value {\n",
       "      tensor {\n",
       "        dtype: DT_INT32\n",
       "        tensor_shape {\n",
       "        }\n",
       "        int_val: 1\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"Const_1\"\n",
       "  op: \"Const\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"value\"\n",
       "    value {\n",
       "      tensor {\n",
       "        dtype: DT_INT32\n",
       "        tensor_shape {\n",
       "        }\n",
       "        int_val: 2\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"Const_2\"\n",
       "  op: \"Const\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"value\"\n",
       "    value {\n",
       "      tensor {\n",
       "        dtype: DT_INT32\n",
       "        tensor_shape {\n",
       "        }\n",
       "        int_val: 3\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"sub\"\n",
       "  op: \"Sub\"\n",
       "  input: \"Const\"\n",
       "  input: \"Const_1\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"mul/x\"\n",
       "  op: \"Const\"\n",
       "  attr {\n",
       "    key: \"dtype\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "  attr {\n",
       "    key: \"value\"\n",
       "    value {\n",
       "      tensor {\n",
       "        dtype: DT_INT32\n",
       "        tensor_shape {\n",
       "        }\n",
       "        int_val: 2\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"mul\"\n",
       "  op: \"Mul\"\n",
       "  input: \"mul/x\"\n",
       "  input: \"sub\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"add\"\n",
       "  op: \"AddV2\"\n",
       "  input: \"mul\"\n",
       "  input: \"Const_2\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "node {\n",
       "  name: \"Identity\"\n",
       "  op: \"Identity\"\n",
       "  input: \"add\"\n",
       "  attr {\n",
       "    key: \"T\"\n",
       "    value {\n",
       "      type: DT_INT32\n",
       "    }\n",
       "  }\n",
       "}\n",
       "versions {\n",
       "  producer: 119\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_func.graph.as_graph_def()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서플로우의 변수\n",
    "\n",
    "텐서 플로에서 변수는 특별한 종류의 텐서 객체이다. 훈련 과정 동안 모델 파라미터를 저장하고 업데이트할 수 있습니다. 예를 들어 신경망의 입력층, 은닉층, 출력층에 있는 가중치입니다. 변수를 정의할 때 초기 텐서 값을 지정해야 한다.\n",
    "\n",
    "텐서플로 변수를 정의하는 방법은 다음과 같다.\n",
    "tf.Variable(<initial-value>, name=<optional-name>)\n",
    "    \n",
    "tf.Variable 에는 shape이나 dtype을 설정할 수 없다. 크기와 타입은 초기값과 동일하게 설정됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'w2:0' shape=(2, 4) dtype=int32, numpy=\n",
      "array([[1, 2, 3, 4],\n",
      "       [5, 6, 7, 8]])>\n"
     ]
    }
   ],
   "source": [
    "w2 = tf.Variable(np.array([[1,2,3,4], [5,6,7,8]]), name = 'w2')\n",
    "print(w2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 3 4 5]\n",
      " [6 7 8 9]], shape=(2, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(w2 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.keras API 자세히 배우기\n",
    "\n",
    "## Sequential 모델\n",
    "\n",
    "sequential 모델은 층을 순서대로 쌓은 모델을 만든다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def make_random_data():\n",
    "    x = np.random.uniform(low = -2, high = 2, size = 200)\n",
    "    y = []\n",
    "    for t in x:\n",
    "        r = np.random.normal(loc = 0.0, scale =(0.5 + t *t/ 3),\n",
    "                            size = None)\n",
    "        y.append(r)\n",
    "    return x, 1.726 * x - 0.84 + np.array(y)\n",
    "\n",
    "x, y = make_random_data()\n",
    "plt.plot(x, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = x[:150], y[:150]\n",
    "x_test, y_test = x[150:], y[150:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=1, input_dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - 0s 1ms/sample - loss: 0.7567 - val_loss: 0.9010\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7573 - val_loss: 0.8997\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7561 - val_loss: 0.8982\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7570 - val_loss: 0.8987\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7559 - val_loss: 0.8986\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.8991\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7569 - val_loss: 0.9033\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7575 - val_loss: 0.9017\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7568 - val_loss: 0.9041\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7570 - val_loss: 0.9016\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7563 - val_loss: 0.9000\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7574 - val_loss: 0.8979\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7568 - val_loss: 0.8976\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7564 - val_loss: 0.8989\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7563 - val_loss: 0.8977\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7559 - val_loss: 0.8969\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7568 - val_loss: 0.9041\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7576 - val_loss: 0.9047\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7579 - val_loss: 0.9047\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7566 - val_loss: 0.9053\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7557 - val_loss: 0.9061\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7557 - val_loss: 0.9084\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7567 - val_loss: 0.9080\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7562 - val_loss: 0.9103\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - 0s 119us/sample - loss: 0.7570 - val_loss: 0.9119\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7570 - val_loss: 0.9078\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7559 - val_loss: 0.9103\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7563 - val_loss: 0.9094\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7559 - val_loss: 0.9095\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7571 - val_loss: 0.9083\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7561 - val_loss: 0.9095\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.7564 - val_loss: 0.9125\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7560 - val_loss: 0.9145\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7564 - val_loss: 0.9119\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7564 - val_loss: 0.9078\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7563 - val_loss: 0.9056\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7562 - val_loss: 0.9046\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7567 - val_loss: 0.9109\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7560 - val_loss: 0.9123\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7563 - val_loss: 0.9120\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.7578 - val_loss: 0.9112\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.9090\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7561 - val_loss: 0.9058\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7561 - val_loss: 0.9056\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7561 - val_loss: 0.9047\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7557 - val_loss: 0.9033\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7569 - val_loss: 0.9055\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7559 - val_loss: 0.9033\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7559 - val_loss: 0.9041\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7576 - val_loss: 0.9062\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7560 - val_loss: 0.9059\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7560 - val_loss: 0.9046\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.7562 - val_loss: 0.9023\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7565 - val_loss: 0.9030\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7563 - val_loss: 0.9029\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7560 - val_loss: 0.9027\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7569 - val_loss: 0.9086\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7564 - val_loss: 0.9098\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7574 - val_loss: 0.9115\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7565 - val_loss: 0.9160\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7571 - val_loss: 0.9195\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.7569 - val_loss: 0.9145\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7565 - val_loss: 0.9153\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7572 - val_loss: 0.9177\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7563 - val_loss: 0.9165\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7570 - val_loss: 0.9219\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7573 - val_loss: 0.9247\n",
      "Epoch 68/100\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.7572 - val_loss: 0.9224\n",
      "Epoch 69/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7570 - val_loss: 0.9196\n",
      "Epoch 70/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7572 - val_loss: 0.9175\n",
      "Epoch 71/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7568 - val_loss: 0.9138\n",
      "Epoch 72/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7558 - val_loss: 0.9133\n",
      "Epoch 73/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7580 - val_loss: 0.9114\n",
      "Epoch 74/100\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.7558 - val_loss: 0.9124\n",
      "Epoch 75/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7568 - val_loss: 0.9070\n",
      "Epoch 76/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7564 - val_loss: 0.9038\n",
      "Epoch 77/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7578 - val_loss: 0.9064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7559 - val_loss: 0.9039\n",
      "Epoch 79/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7561 - val_loss: 0.9023\n",
      "Epoch 80/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7579 - val_loss: 0.9036\n",
      "Epoch 81/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7564 - val_loss: 0.9012\n",
      "Epoch 82/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7567 - val_loss: 0.9033\n",
      "Epoch 83/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7574 - val_loss: 0.9014\n",
      "Epoch 84/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7560 - val_loss: 0.8994\n",
      "Epoch 85/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7564 - val_loss: 0.9030\n",
      "Epoch 86/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7565 - val_loss: 0.9035\n",
      "Epoch 87/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7561 - val_loss: 0.9058\n",
      "Epoch 88/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7567 - val_loss: 0.9045\n",
      "Epoch 89/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7562 - val_loss: 0.9049\n",
      "Epoch 90/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7570 - val_loss: 0.8996\n",
      "Epoch 91/100\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.7563 - val_loss: 0.9015\n",
      "Epoch 92/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7566 - val_loss: 0.9008\n",
      "Epoch 93/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7570 - val_loss: 0.9013\n",
      "Epoch 94/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7562 - val_loss: 0.9004\n",
      "Epoch 95/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7575 - val_loss: 0.9028\n",
      "Epoch 96/100\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7560 - val_loss: 0.8989\n",
      "Epoch 97/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7565 - val_loss: 0.9001\n",
      "Epoch 98/100\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7560 - val_loss: 0.8988\n",
      "Epoch 99/100\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7575 - val_loss: 0.8994\n",
      "Epoch 100/100\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7571 - val_loss: 0.8984\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss ='mse')\n",
    "history = model.fit(x_train, y_train, epochs = 100, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 함수형 API\n",
    "\n",
    "sequential 모델은 층을 순서대로 쌓은 네트워크를 만든다. 이따금 보다 더 복잡한 모델을 만들어야 할 때가 있다. tf.keras의 함수형 APT는 다양한 토폴로지의 네트워크를 만들 수 있습니다. 함수형 api에서는 입력과 출력 사이에 원하는 층을 자유롭게 조합할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model , Input\n",
    "input = tf.keras.Input(shape=(1,))\n",
    "output = tf.keras.layers.Dense(1)(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 튜플로 입력 크기를 지정하여 Input 클래스의 객체를 만듭니다. 그 다음 앞서 보았던 Dense 클래스를 마치 함수처럼 호출하고 있습니다. 클래스 객체를 함수처럼 호출할 수 있는 파이썬의 특수한  __call__() 메서드를 이용한 것이다. 마지막 코드는 다음과 같이 쓴다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(1)\n",
    "output = dense(input)\n",
    "\n",
    "# 또는\n",
    "\n",
    "dense = tf.keras.layers.Dense(1)\n",
    "output = dense.__call__(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Model(input, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/500\n",
      "105/105 [==============================] - 0s 2ms/sample - loss: 4.3373 - val_loss: 2.7028\n",
      "Epoch 2/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 3.7338 - val_loss: 2.3851\n",
      "Epoch 3/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 3.2105 - val_loss: 2.1108\n",
      "Epoch 4/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 2.7629 - val_loss: 1.8896\n",
      "Epoch 5/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 2.3969 - val_loss: 1.6812\n",
      "Epoch 6/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 2.0617 - val_loss: 1.5483\n",
      "Epoch 7/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 1.8256 - val_loss: 1.4333\n",
      "Epoch 8/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 1.6303 - val_loss: 1.3356\n",
      "Epoch 9/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 1.4646 - val_loss: 1.2334\n",
      "Epoch 10/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 1.2967 - val_loss: 1.1716\n",
      "Epoch 11/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 1.1935 - val_loss: 1.1135\n",
      "Epoch 12/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 1.1048 - val_loss: 1.0672\n",
      "Epoch 13/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 1.0326 - val_loss: 1.0400\n",
      "Epoch 14/500\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 0.9803 - val_loss: 1.0090\n",
      "Epoch 15/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.9376 - val_loss: 0.9888\n",
      "Epoch 16/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.9030 - val_loss: 0.9683\n",
      "Epoch 17/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.8698 - val_loss: 0.9555\n",
      "Epoch 18/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.8478 - val_loss: 0.9444\n",
      "Epoch 19/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.8308 - val_loss: 0.9350\n",
      "Epoch 20/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.8180 - val_loss: 0.9434\n",
      "Epoch 21/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.8118 - val_loss: 0.9330\n",
      "Epoch 22/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.8038 - val_loss: 0.9352\n",
      "Epoch 23/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7979 - val_loss: 0.9258\n",
      "Epoch 24/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7870 - val_loss: 0.9187\n",
      "Epoch 25/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7804 - val_loss: 0.9192\n",
      "Epoch 26/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7771 - val_loss: 0.9131\n",
      "Epoch 27/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7731 - val_loss: 0.9103\n",
      "Epoch 28/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7696 - val_loss: 0.9093\n",
      "Epoch 29/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7675 - val_loss: 0.9042\n",
      "Epoch 30/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7654 - val_loss: 0.9045\n",
      "Epoch 31/500\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 0.7666 - val_loss: 0.9027\n",
      "Epoch 32/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7637 - val_loss: 0.9042\n",
      "Epoch 33/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7638 - val_loss: 0.9023\n",
      "Epoch 34/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7621 - val_loss: 0.9024\n",
      "Epoch 35/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7613 - val_loss: 0.8990\n",
      "Epoch 36/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7593 - val_loss: 0.9015\n",
      "Epoch 37/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7601 - val_loss: 0.9031\n",
      "Epoch 38/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7626 - val_loss: 0.9067\n",
      "Epoch 39/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7598 - val_loss: 0.9096\n",
      "Epoch 40/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7597 - val_loss: 0.9027\n",
      "Epoch 41/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7598 - val_loss: 0.9032\n",
      "Epoch 42/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7577 - val_loss: 0.9040\n",
      "Epoch 43/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7566 - val_loss: 0.8988\n",
      "Epoch 44/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7572 - val_loss: 0.8989\n",
      "Epoch 45/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7559 - val_loss: 0.8984\n",
      "Epoch 46/500\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.7563 - val_loss: 0.8975\n",
      "Epoch 47/500\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 0.7576 - val_loss: 0.9007\n",
      "Epoch 48/500\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 0.7563 - val_loss: 0.9045\n",
      "Epoch 49/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7558 - val_loss: 0.9045\n",
      "Epoch 50/500\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 0.7564 - val_loss: 0.9023\n",
      "Epoch 51/500\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 0.7557 - val_loss: 0.9006\n",
      "Epoch 52/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7556 - val_loss: 0.9009\n",
      "Epoch 53/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7561 - val_loss: 0.8997\n",
      "Epoch 54/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7572 - val_loss: 0.8984\n",
      "Epoch 55/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7562 - val_loss: 0.9004\n",
      "Epoch 56/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7570 - val_loss: 0.9035\n",
      "Epoch 57/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7558 - val_loss: 0.9026\n",
      "Epoch 58/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7565 - val_loss: 0.9014\n",
      "Epoch 59/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7579 - val_loss: 0.9027\n",
      "Epoch 60/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7559 - val_loss: 0.9019\n",
      "Epoch 61/500\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 0.7570 - val_loss: 0.9074\n",
      "Epoch 62/500\n",
      "105/105 [==============================] - 0s 129us/sample - loss: 0.7560 - val_loss: 0.9056\n",
      "Epoch 63/500\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 0.7559 - val_loss: 0.9029\n",
      "Epoch 64/500\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 0.7562 - val_loss: 0.9038\n",
      "Epoch 65/500\n",
      "105/105 [==============================] - 0s 161us/sample - loss: 0.7578 - val_loss: 0.9003\n",
      "Epoch 66/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7560 - val_loss: 0.9050\n",
      "Epoch 67/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7577 - val_loss: 0.9045\n",
      "Epoch 68/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7567 - val_loss: 0.9020\n",
      "Epoch 69/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7575 - val_loss: 0.9043\n",
      "Epoch 70/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7556 - val_loss: 0.9060\n",
      "Epoch 71/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7571 - val_loss: 0.9068\n",
      "Epoch 72/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7561 - val_loss: 0.9046\n",
      "Epoch 73/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7559 - val_loss: 0.9026\n",
      "Epoch 74/500\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.699 - 0s 114us/sample - loss: 0.7563 - val_loss: 0.8993\n",
      "Epoch 75/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7559 - val_loss: 0.9001\n",
      "Epoch 76/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7562 - val_loss: 0.8975\n",
      "Epoch 77/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7561 - val_loss: 0.8979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.8976\n",
      "Epoch 79/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7565 - val_loss: 0.8988\n",
      "Epoch 80/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7581 - val_loss: 0.8998\n",
      "Epoch 81/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7562 - val_loss: 0.8982\n",
      "Epoch 82/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7563 - val_loss: 0.8964\n",
      "Epoch 83/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7569 - val_loss: 0.8928\n",
      "Epoch 84/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7570 - val_loss: 0.8975\n",
      "Epoch 85/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7576 - val_loss: 0.8958\n",
      "Epoch 86/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7578 - val_loss: 0.8953\n",
      "Epoch 87/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7575 - val_loss: 0.8991\n",
      "Epoch 88/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7566 - val_loss: 0.9013\n",
      "Epoch 89/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7571 - val_loss: 0.8966\n",
      "Epoch 90/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7570 - val_loss: 0.8994\n",
      "Epoch 91/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7560 - val_loss: 0.9002\n",
      "Epoch 92/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7601 - val_loss: 0.9009\n",
      "Epoch 93/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7569 - val_loss: 0.9030\n",
      "Epoch 94/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7569 - val_loss: 0.8992\n",
      "Epoch 95/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7565 - val_loss: 0.9004\n",
      "Epoch 96/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7575 - val_loss: 0.9011\n",
      "Epoch 97/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7563 - val_loss: 0.8995\n",
      "Epoch 98/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7567 - val_loss: 0.9045\n",
      "Epoch 99/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7572 - val_loss: 0.9026\n",
      "Epoch 100/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7586 - val_loss: 0.9021\n",
      "Epoch 101/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7571 - val_loss: 0.9072\n",
      "Epoch 102/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7576 - val_loss: 0.9085\n",
      "Epoch 103/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7585 - val_loss: 0.9076\n",
      "Epoch 104/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7580 - val_loss: 0.9080\n",
      "Epoch 105/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7568 - val_loss: 0.9052\n",
      "Epoch 106/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7577 - val_loss: 0.9035\n",
      "Epoch 107/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7576 - val_loss: 0.9073\n",
      "Epoch 108/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7573 - val_loss: 0.9036\n",
      "Epoch 109/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7577 - val_loss: 0.9036\n",
      "Epoch 110/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7563 - val_loss: 0.9038\n",
      "Epoch 111/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7564 - val_loss: 0.9082\n",
      "Epoch 112/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7558 - val_loss: 0.9076\n",
      "Epoch 113/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7558 - val_loss: 0.9089\n",
      "Epoch 114/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7565 - val_loss: 0.9128\n",
      "Epoch 115/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7576 - val_loss: 0.9118\n",
      "Epoch 116/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.9097\n",
      "Epoch 117/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7560 - val_loss: 0.9082\n",
      "Epoch 118/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7564 - val_loss: 0.9106\n",
      "Epoch 119/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7562 - val_loss: 0.9092\n",
      "Epoch 120/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7563 - val_loss: 0.9086\n",
      "Epoch 121/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.9120\n",
      "Epoch 122/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7563 - val_loss: 0.9157\n",
      "Epoch 123/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7578 - val_loss: 0.9113\n",
      "Epoch 124/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7586 - val_loss: 0.9131\n",
      "Epoch 125/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7587 - val_loss: 0.9120\n",
      "Epoch 126/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7560 - val_loss: 0.9115\n",
      "Epoch 127/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7567 - val_loss: 0.9088\n",
      "Epoch 128/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7566 - val_loss: 0.9035\n",
      "Epoch 129/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7563 - val_loss: 0.9040\n",
      "Epoch 130/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7580 - val_loss: 0.9075\n",
      "Epoch 131/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7563 - val_loss: 0.9122\n",
      "Epoch 132/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7563 - val_loss: 0.9116\n",
      "Epoch 133/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7559 - val_loss: 0.9099\n",
      "Epoch 134/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7583 - val_loss: 0.9015\n",
      "Epoch 135/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7573 - val_loss: 0.9023\n",
      "Epoch 136/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7570 - val_loss: 0.9051\n",
      "Epoch 137/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7564 - val_loss: 0.9053\n",
      "Epoch 138/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7574 - val_loss: 0.9049\n",
      "Epoch 139/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7561 - val_loss: 0.9006\n",
      "Epoch 140/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7583 - val_loss: 0.9024\n",
      "Epoch 141/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7569 - val_loss: 0.9048\n",
      "Epoch 142/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7563 - val_loss: 0.9061\n",
      "Epoch 143/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7561 - val_loss: 0.9072\n",
      "Epoch 144/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.9124\n",
      "Epoch 145/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7567 - val_loss: 0.9129\n",
      "Epoch 146/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7567 - val_loss: 0.9173\n",
      "Epoch 147/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7571 - val_loss: 0.9128\n",
      "Epoch 148/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7569 - val_loss: 0.9117\n",
      "Epoch 149/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7564 - val_loss: 0.9078\n",
      "Epoch 150/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7564 - val_loss: 0.9081\n",
      "Epoch 151/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7561 - val_loss: 0.9061\n",
      "Epoch 152/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7566 - val_loss: 0.9094\n",
      "Epoch 153/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7577 - val_loss: 0.9090\n",
      "Epoch 154/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7574 - val_loss: 0.9076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7564 - val_loss: 0.9039\n",
      "Epoch 156/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7559 - val_loss: 0.9021\n",
      "Epoch 157/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7560 - val_loss: 0.9044\n",
      "Epoch 158/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7567 - val_loss: 0.9032\n",
      "Epoch 159/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7563 - val_loss: 0.9044\n",
      "Epoch 160/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7576 - val_loss: 0.9011\n",
      "Epoch 161/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.8958\n",
      "Epoch 162/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7568 - val_loss: 0.8987\n",
      "Epoch 163/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7563 - val_loss: 0.9027\n",
      "Epoch 164/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7564 - val_loss: 0.9039\n",
      "Epoch 165/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7559 - val_loss: 0.9057\n",
      "Epoch 166/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7556 - val_loss: 0.9067\n",
      "Epoch 167/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7573 - val_loss: 0.9131\n",
      "Epoch 168/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7585 - val_loss: 0.9124\n",
      "Epoch 169/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7567 - val_loss: 0.9115\n",
      "Epoch 170/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7564 - val_loss: 0.9097\n",
      "Epoch 171/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7560 - val_loss: 0.9090\n",
      "Epoch 172/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7561 - val_loss: 0.9078\n",
      "Epoch 173/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7557 - val_loss: 0.9069\n",
      "Epoch 174/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7582 - val_loss: 0.9008\n",
      "Epoch 175/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7558 - val_loss: 0.8987\n",
      "Epoch 176/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7579 - val_loss: 0.9036\n",
      "Epoch 177/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7575 - val_loss: 0.9036\n",
      "Epoch 178/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7561 - val_loss: 0.9012\n",
      "Epoch 179/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7562 - val_loss: 0.9037\n",
      "Epoch 180/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.9056\n",
      "Epoch 181/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7558 - val_loss: 0.9041\n",
      "Epoch 182/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7560 - val_loss: 0.9011\n",
      "Epoch 183/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7565 - val_loss: 0.8984\n",
      "Epoch 184/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7565 - val_loss: 0.8974\n",
      "Epoch 185/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7567 - val_loss: 0.8987\n",
      "Epoch 186/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7568 - val_loss: 0.9035\n",
      "Epoch 187/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7563 - val_loss: 0.9014\n",
      "Epoch 188/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7591 - val_loss: 0.9018\n",
      "Epoch 189/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.9026\n",
      "Epoch 190/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7560 - val_loss: 0.9027\n",
      "Epoch 191/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7557 - val_loss: 0.8994\n",
      "Epoch 192/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7565 - val_loss: 0.8996\n",
      "Epoch 193/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7562 - val_loss: 0.8955\n",
      "Epoch 194/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7566 - val_loss: 0.8957\n",
      "Epoch 195/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7571 - val_loss: 0.8917\n",
      "Epoch 196/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7582 - val_loss: 0.8919\n",
      "Epoch 197/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7575 - val_loss: 0.8917\n",
      "Epoch 198/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7568 - val_loss: 0.8937\n",
      "Epoch 199/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7575 - val_loss: 0.8934\n",
      "Epoch 200/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7568 - val_loss: 0.8926\n",
      "Epoch 201/500\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.945 - 0s 114us/sample - loss: 0.7567 - val_loss: 0.8909\n",
      "Epoch 202/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7569 - val_loss: 0.8918\n",
      "Epoch 203/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7568 - val_loss: 0.8932\n",
      "Epoch 204/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7573 - val_loss: 0.8936\n",
      "Epoch 205/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7572 - val_loss: 0.8939\n",
      "Epoch 206/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7584 - val_loss: 0.8969\n",
      "Epoch 207/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7564 - val_loss: 0.8962\n",
      "Epoch 208/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7574 - val_loss: 0.8964\n",
      "Epoch 209/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7580 - val_loss: 0.8971\n",
      "Epoch 210/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7568 - val_loss: 0.8961\n",
      "Epoch 211/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7576 - val_loss: 0.8973\n",
      "Epoch 212/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7568 - val_loss: 0.8981\n",
      "Epoch 213/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7564 - val_loss: 0.8955\n",
      "Epoch 214/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7579 - val_loss: 0.8976\n",
      "Epoch 215/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7565 - val_loss: 0.9005\n",
      "Epoch 216/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7570 - val_loss: 0.9001\n",
      "Epoch 217/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7563 - val_loss: 0.9002\n",
      "Epoch 218/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7571 - val_loss: 0.9057\n",
      "Epoch 219/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7568 - val_loss: 0.9038\n",
      "Epoch 220/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7559 - val_loss: 0.9030\n",
      "Epoch 221/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7566 - val_loss: 0.9061\n",
      "Epoch 222/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7561 - val_loss: 0.9069\n",
      "Epoch 223/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7558 - val_loss: 0.9103\n",
      "Epoch 224/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.9142\n",
      "Epoch 225/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7573 - val_loss: 0.9133\n",
      "Epoch 226/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7561 - val_loss: 0.9126\n",
      "Epoch 227/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7563 - val_loss: 0.9131\n",
      "Epoch 228/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7564 - val_loss: 0.9126\n",
      "Epoch 229/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7566 - val_loss: 0.9144\n",
      "Epoch 230/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7563 - val_loss: 0.9102\n",
      "Epoch 231/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7572 - val_loss: 0.9080\n",
      "Epoch 232/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7575 - val_loss: 0.9049\n",
      "Epoch 233/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7566 - val_loss: 0.9097\n",
      "Epoch 234/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7568 - val_loss: 0.9058\n",
      "Epoch 235/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7567 - val_loss: 0.9110\n",
      "Epoch 236/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7571 - val_loss: 0.9104\n",
      "Epoch 237/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7566 - val_loss: 0.9084\n",
      "Epoch 238/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.9074\n",
      "Epoch 239/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7557 - val_loss: 0.9075\n",
      "Epoch 240/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7557 - val_loss: 0.9049\n",
      "Epoch 241/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7568 - val_loss: 0.9015\n",
      "Epoch 242/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7565 - val_loss: 0.9000\n",
      "Epoch 243/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7567 - val_loss: 0.8972\n",
      "Epoch 244/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7564 - val_loss: 0.8964\n",
      "Epoch 245/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7569 - val_loss: 0.9015\n",
      "Epoch 246/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7568 - val_loss: 0.8983\n",
      "Epoch 247/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7571 - val_loss: 0.8948\n",
      "Epoch 248/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7567 - val_loss: 0.8933\n",
      "Epoch 249/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7576 - val_loss: 0.9003\n",
      "Epoch 250/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7567 - val_loss: 0.8992\n",
      "Epoch 251/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7561 - val_loss: 0.9034\n",
      "Epoch 252/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7565 - val_loss: 0.9061\n",
      "Epoch 253/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7570 - val_loss: 0.9077\n",
      "Epoch 254/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7563 - val_loss: 0.9073\n",
      "Epoch 255/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7557 - val_loss: 0.9090\n",
      "Epoch 256/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7563 - val_loss: 0.9080\n",
      "Epoch 257/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7560 - val_loss: 0.9125\n",
      "Epoch 258/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7567 - val_loss: 0.9097\n",
      "Epoch 259/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7569 - val_loss: 0.9066\n",
      "Epoch 260/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7563 - val_loss: 0.9094\n",
      "Epoch 261/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7564 - val_loss: 0.9081\n",
      "Epoch 262/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7566 - val_loss: 0.9129\n",
      "Epoch 263/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7577 - val_loss: 0.9117\n",
      "Epoch 264/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7569 - val_loss: 0.9118\n",
      "Epoch 265/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7580 - val_loss: 0.9080\n",
      "Epoch 266/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7565 - val_loss: 0.9049\n",
      "Epoch 267/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7567 - val_loss: 0.9019\n",
      "Epoch 268/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7565 - val_loss: 0.9037\n",
      "Epoch 269/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7569 - val_loss: 0.9023\n",
      "Epoch 270/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7570 - val_loss: 0.9040\n",
      "Epoch 271/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7564 - val_loss: 0.9068\n",
      "Epoch 272/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7574 - val_loss: 0.9064\n",
      "Epoch 273/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7578 - val_loss: 0.9091\n",
      "Epoch 274/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7566 - val_loss: 0.9057\n",
      "Epoch 275/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7572 - val_loss: 0.9023\n",
      "Epoch 276/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7559 - val_loss: 0.9071\n",
      "Epoch 277/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7564 - val_loss: 0.9060\n",
      "Epoch 278/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7578 - val_loss: 0.9031\n",
      "Epoch 279/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7582 - val_loss: 0.9051\n",
      "Epoch 280/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.9081\n",
      "Epoch 281/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7558 - val_loss: 0.9086\n",
      "Epoch 282/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7563 - val_loss: 0.9078\n",
      "Epoch 283/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7566 - val_loss: 0.9077\n",
      "Epoch 284/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7563 - val_loss: 0.9094\n",
      "Epoch 285/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7563 - val_loss: 0.9057\n",
      "Epoch 286/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7564 - val_loss: 0.9107\n",
      "Epoch 287/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7560 - val_loss: 0.9114\n",
      "Epoch 288/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7565 - val_loss: 0.9113\n",
      "Epoch 289/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7564 - val_loss: 0.9106\n",
      "Epoch 290/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7566 - val_loss: 0.9097\n",
      "Epoch 291/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7567 - val_loss: 0.9073\n",
      "Epoch 292/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7564 - val_loss: 0.9133\n",
      "Epoch 293/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7569 - val_loss: 0.9107\n",
      "Epoch 294/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7563 - val_loss: 0.9116\n",
      "Epoch 295/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7559 - val_loss: 0.9118\n",
      "Epoch 296/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7566 - val_loss: 0.9057\n",
      "Epoch 297/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7564 - val_loss: 0.9071\n",
      "Epoch 298/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.9029\n",
      "Epoch 299/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7566 - val_loss: 0.8999\n",
      "Epoch 300/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7563 - val_loss: 0.9001\n",
      "Epoch 301/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7580 - val_loss: 0.9021\n",
      "Epoch 302/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7558 - val_loss: 0.9055\n",
      "Epoch 303/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7560 - val_loss: 0.9072\n",
      "Epoch 304/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7562 - val_loss: 0.9110\n",
      "Epoch 305/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7565 - val_loss: 0.9096\n",
      "Epoch 306/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7568 - val_loss: 0.9061\n",
      "Epoch 307/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7563 - val_loss: 0.9031\n",
      "Epoch 308/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7568 - val_loss: 0.9008\n",
      "Epoch 309/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7563 - val_loss: 0.9056\n",
      "Epoch 310/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7555 - val_loss: 0.9060\n",
      "Epoch 311/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7558 - val_loss: 0.9071\n",
      "Epoch 312/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7566 - val_loss: 0.9067\n",
      "Epoch 313/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7569 - val_loss: 0.9089\n",
      "Epoch 314/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7562 - val_loss: 0.9131\n",
      "Epoch 315/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7563 - val_loss: 0.9097\n",
      "Epoch 316/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7587 - val_loss: 0.9108\n",
      "Epoch 317/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7572 - val_loss: 0.9071\n",
      "Epoch 318/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7564 - val_loss: 0.9045\n",
      "Epoch 319/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7567 - val_loss: 0.9070\n",
      "Epoch 320/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.9083\n",
      "Epoch 321/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7579 - val_loss: 0.9075\n",
      "Epoch 322/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7568 - val_loss: 0.9081\n",
      "Epoch 323/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7582 - val_loss: 0.9066\n",
      "Epoch 324/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7586 - val_loss: 0.9044\n",
      "Epoch 325/500\n",
      "105/105 [==============================] - 0s 115us/sample - loss: 0.7570 - val_loss: 0.9069\n",
      "Epoch 326/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7573 - val_loss: 0.9090\n",
      "Epoch 327/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7568 - val_loss: 0.9123\n",
      "Epoch 328/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7562 - val_loss: 0.9128\n",
      "Epoch 329/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7576 - val_loss: 0.9123\n",
      "Epoch 330/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7587 - val_loss: 0.9126\n",
      "Epoch 331/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7566 - val_loss: 0.9120\n",
      "Epoch 332/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7568 - val_loss: 0.9109\n",
      "Epoch 333/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7561 - val_loss: 0.9081\n",
      "Epoch 334/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7559 - val_loss: 0.9094\n",
      "Epoch 335/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7568 - val_loss: 0.9077\n",
      "Epoch 336/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7571 - val_loss: 0.9093\n",
      "Epoch 337/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7561 - val_loss: 0.9101\n",
      "Epoch 338/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7559 - val_loss: 0.9103\n",
      "Epoch 339/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7567 - val_loss: 0.9111\n",
      "Epoch 340/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7572 - val_loss: 0.9081\n",
      "Epoch 341/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7560 - val_loss: 0.9078\n",
      "Epoch 342/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7560 - val_loss: 0.9090\n",
      "Epoch 343/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7558 - val_loss: 0.9088\n",
      "Epoch 344/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7557 - val_loss: 0.9093\n",
      "Epoch 345/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7564 - val_loss: 0.9069\n",
      "Epoch 346/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7568 - val_loss: 0.9019\n",
      "Epoch 347/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7559 - val_loss: 0.9012\n",
      "Epoch 348/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7568 - val_loss: 0.9033\n",
      "Epoch 349/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7564 - val_loss: 0.9056\n",
      "Epoch 350/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7556 - val_loss: 0.9074\n",
      "Epoch 351/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7559 - val_loss: 0.9075\n",
      "Epoch 352/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7560 - val_loss: 0.9079\n",
      "Epoch 353/500\n",
      "105/105 [==============================] - 0s 112us/sample - loss: 0.7581 - val_loss: 0.9086\n",
      "Epoch 354/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.9128\n",
      "Epoch 355/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7574 - val_loss: 0.9088\n",
      "Epoch 356/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7556 - val_loss: 0.9065\n",
      "Epoch 357/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7566 - val_loss: 0.9097\n",
      "Epoch 358/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7573 - val_loss: 0.9091\n",
      "Epoch 359/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7561 - val_loss: 0.9093\n",
      "Epoch 360/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7564 - val_loss: 0.9043\n",
      "Epoch 361/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7567 - val_loss: 0.9056\n",
      "Epoch 362/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7569 - val_loss: 0.8996\n",
      "Epoch 363/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7560 - val_loss: 0.9002\n",
      "Epoch 364/500\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 0.7567 - val_loss: 0.9034\n",
      "Epoch 365/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7585 - val_loss: 0.9059\n",
      "Epoch 366/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7565 - val_loss: 0.9083\n",
      "Epoch 367/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7570 - val_loss: 0.9089\n",
      "Epoch 368/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.9082\n",
      "Epoch 369/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7569 - val_loss: 0.9067\n",
      "Epoch 370/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7564 - val_loss: 0.9040\n",
      "Epoch 371/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7562 - val_loss: 0.9030\n",
      "Epoch 372/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7563 - val_loss: 0.9009\n",
      "Epoch 373/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7563 - val_loss: 0.9001\n",
      "Epoch 374/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7556 - val_loss: 0.8987\n",
      "Epoch 375/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7559 - val_loss: 0.8976\n",
      "Epoch 376/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7583 - val_loss: 0.8968\n",
      "Epoch 377/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7565 - val_loss: 0.9023\n",
      "Epoch 378/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7566 - val_loss: 0.9006\n",
      "Epoch 379/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7568 - val_loss: 0.8992\n",
      "Epoch 380/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7562 - val_loss: 0.9005\n",
      "Epoch 381/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7561 - val_loss: 0.8986\n",
      "Epoch 382/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7568 - val_loss: 0.9013\n",
      "Epoch 383/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7575 - val_loss: 0.8978\n",
      "Epoch 384/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7561 - val_loss: 0.8979\n",
      "Epoch 385/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7562 - val_loss: 0.8983\n",
      "Epoch 386/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7565 - val_loss: 0.9011\n",
      "Epoch 387/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7563 - val_loss: 0.9008\n",
      "Epoch 388/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7581 - val_loss: 0.8975\n",
      "Epoch 389/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7562 - val_loss: 0.8980\n",
      "Epoch 390/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7569 - val_loss: 0.8961\n",
      "Epoch 391/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7577 - val_loss: 0.8996\n",
      "Epoch 392/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.8987\n",
      "Epoch 393/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7574 - val_loss: 0.8939\n",
      "Epoch 394/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7582 - val_loss: 0.8953\n",
      "Epoch 395/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7570 - val_loss: 0.8937\n",
      "Epoch 396/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7574 - val_loss: 0.8955\n",
      "Epoch 397/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7563 - val_loss: 0.8971\n",
      "Epoch 398/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7580 - val_loss: 0.8960\n",
      "Epoch 399/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7561 - val_loss: 0.8973\n",
      "Epoch 400/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.8994\n",
      "Epoch 401/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7572 - val_loss: 0.9021\n",
      "Epoch 402/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7567 - val_loss: 0.9032\n",
      "Epoch 403/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.9026\n",
      "Epoch 404/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7561 - val_loss: 0.9070\n",
      "Epoch 405/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7573 - val_loss: 0.9094\n",
      "Epoch 406/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7577 - val_loss: 0.9092\n",
      "Epoch 407/500\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.7577 - val_loss: 0.9077\n",
      "Epoch 408/500\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.7569 - val_loss: 0.9117\n",
      "Epoch 409/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7573 - val_loss: 0.9121\n",
      "Epoch 410/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7579 - val_loss: 0.9126\n",
      "Epoch 411/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7574 - val_loss: 0.9115\n",
      "Epoch 412/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7572 - val_loss: 0.9080\n",
      "Epoch 413/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7576 - val_loss: 0.9043\n",
      "Epoch 414/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7577 - val_loss: 0.9036\n",
      "Epoch 415/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7564 - val_loss: 0.9031\n",
      "Epoch 416/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7584 - val_loss: 0.8987\n",
      "Epoch 417/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7568 - val_loss: 0.8983\n",
      "Epoch 418/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7571 - val_loss: 0.8973\n",
      "Epoch 419/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7573 - val_loss: 0.8998\n",
      "Epoch 420/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7577 - val_loss: 0.9073\n",
      "Epoch 421/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7588 - val_loss: 0.9121\n",
      "Epoch 422/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7594 - val_loss: 0.9112\n",
      "Epoch 423/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7575 - val_loss: 0.9107\n",
      "Epoch 424/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7571 - val_loss: 0.9079\n",
      "Epoch 425/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7574 - val_loss: 0.9059\n",
      "Epoch 426/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7580 - val_loss: 0.9044\n",
      "Epoch 427/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7577 - val_loss: 0.9061\n",
      "Epoch 428/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7559 - val_loss: 0.9046\n",
      "Epoch 429/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7568 - val_loss: 0.9001\n",
      "Epoch 430/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7578 - val_loss: 0.9038\n",
      "Epoch 431/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7567 - val_loss: 0.9108\n",
      "Epoch 432/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7574 - val_loss: 0.9121\n",
      "Epoch 433/500\n",
      "105/105 [==============================] - 0s 124us/sample - loss: 0.7562 - val_loss: 0.9120\n",
      "Epoch 434/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7569 - val_loss: 0.9160\n",
      "Epoch 435/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7565 - val_loss: 0.9184\n",
      "Epoch 436/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7568 - val_loss: 0.9176\n",
      "Epoch 437/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7580 - val_loss: 0.9185\n",
      "Epoch 438/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7575 - val_loss: 0.9133\n",
      "Epoch 439/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7560 - val_loss: 0.9108\n",
      "Epoch 440/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7562 - val_loss: 0.9098\n",
      "Epoch 441/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7568 - val_loss: 0.9079\n",
      "Epoch 442/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7574 - val_loss: 0.9062\n",
      "Epoch 443/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7561 - val_loss: 0.9089\n",
      "Epoch 444/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7558 - val_loss: 0.9079\n",
      "Epoch 445/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7577 - val_loss: 0.9056\n",
      "Epoch 446/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7562 - val_loss: 0.9038\n",
      "Epoch 447/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7567 - val_loss: 0.9029\n",
      "Epoch 448/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7567 - val_loss: 0.9047\n",
      "Epoch 449/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7562 - val_loss: 0.9082\n",
      "Epoch 450/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7559 - val_loss: 0.9077\n",
      "Epoch 451/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7559 - val_loss: 0.9086\n",
      "Epoch 452/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7564 - val_loss: 0.9152\n",
      "Epoch 453/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7563 - val_loss: 0.9166\n",
      "Epoch 454/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7567 - val_loss: 0.9220\n",
      "Epoch 455/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7578 - val_loss: 0.9268\n",
      "Epoch 456/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7578 - val_loss: 0.9273\n",
      "Epoch 457/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7577 - val_loss: 0.9236\n",
      "Epoch 458/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7580 - val_loss: 0.9255\n",
      "Epoch 459/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7595 - val_loss: 0.9249\n",
      "Epoch 460/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7594 - val_loss: 0.9222\n",
      "Epoch 461/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7575 - val_loss: 0.9215\n",
      "Epoch 462/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7577 - val_loss: 0.9249\n",
      "Epoch 463/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7575 - val_loss: 0.9186\n",
      "Epoch 464/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7572 - val_loss: 0.9173\n",
      "Epoch 465/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7569 - val_loss: 0.9158\n",
      "Epoch 466/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7565 - val_loss: 0.9201\n",
      "Epoch 467/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7573 - val_loss: 0.9226\n",
      "Epoch 468/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7577 - val_loss: 0.9231\n",
      "Epoch 469/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7593 - val_loss: 0.9201\n",
      "Epoch 470/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7579 - val_loss: 0.9221\n",
      "Epoch 471/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7577 - val_loss: 0.9174\n",
      "Epoch 472/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7574 - val_loss: 0.9170\n",
      "Epoch 473/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7567 - val_loss: 0.9124\n",
      "Epoch 474/500\n",
      "105/105 [==============================] - 0s 133us/sample - loss: 0.7572 - val_loss: 0.9083\n",
      "Epoch 475/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7564 - val_loss: 0.9141\n",
      "Epoch 476/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7565 - val_loss: 0.9136\n",
      "Epoch 477/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7565 - val_loss: 0.9133\n",
      "Epoch 478/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7568 - val_loss: 0.9198\n",
      "Epoch 479/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7592 - val_loss: 0.9193\n",
      "Epoch 480/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7569 - val_loss: 0.9240\n",
      "Epoch 481/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7579 - val_loss: 0.9239\n",
      "Epoch 482/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7608 - val_loss: 0.9258\n",
      "Epoch 483/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7578 - val_loss: 0.9251\n",
      "Epoch 484/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7581 - val_loss: 0.9210\n",
      "Epoch 485/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7572 - val_loss: 0.9174\n",
      "Epoch 486/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7586 - val_loss: 0.9152\n",
      "Epoch 487/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7570 - val_loss: 0.9114\n",
      "Epoch 488/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7566 - val_loss: 0.9099\n",
      "Epoch 489/500\n",
      "105/105 [==============================] - 0s 104us/sample - loss: 0.7560 - val_loss: 0.9091\n",
      "Epoch 490/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7567 - val_loss: 0.9094\n",
      "Epoch 491/500\n",
      "105/105 [==============================] - 0s 190us/sample - loss: 0.7563 - val_loss: 0.9043\n",
      "Epoch 492/500\n",
      "105/105 [==============================] - 0s 142us/sample - loss: 0.7564 - val_loss: 0.9043\n",
      "Epoch 493/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7562 - val_loss: 0.9043\n",
      "Epoch 494/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7559 - val_loss: 0.9037\n",
      "Epoch 495/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7564 - val_loss: 0.9095\n",
      "Epoch 496/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7563 - val_loss: 0.9088\n",
      "Epoch 497/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7558 - val_loss: 0.9088\n",
      "Epoch 498/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7571 - val_loss: 0.9047\n",
      "Epoch 499/500\n",
      "105/105 [==============================] - 0s 114us/sample - loss: 0.7567 - val_loss: 0.9025\n",
      "Epoch 500/500\n",
      "105/105 [==============================] - 0s 123us/sample - loss: 0.7562 - val_loss: 0.9040\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss='mse')\n",
    "history = model.fit(x_train, y_train, epochs = 500, validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
