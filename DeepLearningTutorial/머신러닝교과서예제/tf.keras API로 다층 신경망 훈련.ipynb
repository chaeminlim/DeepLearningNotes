{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "(X_train, Y_train) , (X_test, Y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vals = np.mean(X_train, axis = 0)\n",
    "std_val = np.std(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_centered = (X_train - mean_vals) / std_val\n",
    "X_test_centered = (X_test - mean_vals) / std_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train_centered = X_train_centered.reshape((X_train_centered.shape[0], -1)) \n",
    "X_test_centered = X_test_centered.reshape((X_test_centered.shape[0], -1)) \n",
    "\n",
    "print(X_train_centered.shape, Y_train.shape)\n",
    "print(X_test_centered.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터를 준비하기 위해 클레스 레이블  0 ~ 9 사이 정수를 원 핫 인코딩으로 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처음 3개 레이블: [5 0 4]\n"
     ]
    }
   ],
   "source": [
    "Y_train_onehot = tf.keras.utils.to_categorical(Y_train)\n",
    "print('처음 3개 레이블:', Y_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처음 3개 레이블 원 핫\n",
      ": [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('처음 3개 레이블 원 핫\\n:', Y_train_onehot[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 피드 포워드 신경망 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 신경망을 구현한다. 간단하게 세 개의 완전 연결 층을 만든다.\n",
    "처음 두개의 층은 하이퍼볼릭 탄젠트 활성 함수를 가진 50개의 은닉 유닛으로 이루어진다.\n",
    "마지막 층은 열개의 클래스에 레이블에 해당하는 열개의 은닉 유닛을 가진다. 마지막 층은 각 클래스의 확률을 계산하기 위해 소프트맥스 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units = 50,\n",
    "        input_dim = X_train_centered.shape[1],\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='tanh'))\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units = 50,\n",
    "        input_dim = 50,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='tanh'))\n",
    "model.add(\n",
    "tf.keras.layers.Dense(\n",
    "units=Y_train_onehot.shape[1],\n",
    "input_dim = 50,\n",
    "kernel_initializer = 'glorot_uniform',\n",
    "bias_initializer='zeros',\n",
    "activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 Sequential 클래스를 사용하여 피드 포워드 신경망을 구현하는 새로운 모델을 초기화한다.\n",
    "두개의 연속된 층에서 unit과 입력 유닛이 일치해야 한다. units, input_dim\n",
    "\n",
    "Note. kernel_initializer = 'glorot_uniform'은 새로운 가중치 행렬 초기화 알고리즘 이다. \n",
    "글로럿세이비어 초기화는 심층 신경망을 안정적으로 초기화하는 방법이다. 절편은 일반적으로 0으로 초기화한다. \n",
    "\n",
    "지금까지 만든 모델 구조를 summary() 메소드를 사용하여 출력한다. 출력 시작 부분이 신경망 입력에 가까운 층이고 끝부분이 출력에 가까운 층이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 42,310\n",
      "Trainable params: 42,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 피드포워드 신경망 훈련\n",
    "\n",
    "모델 구성을 마치면 훈련을 수행하기 전에 모델을 컴파일 해야 한다.\n",
    "이 단계에서 최적화할 손실 함수를 정의하고 최적화에 사용할 경사 하강법 옵티마이저를 선택한다. 이전 장에서 사용해 보았던 확률적 경사 학아법 최적화를 선택한다. 에포크마다 학습률을 조절하기 위한 학습률 감쇠 상수와 모멘텀 값을 지정한다. 마지막으로 비용 함수를 catergorical_crossentropy로 설정한다.\n",
    "\n",
    "이진 크로스 엔트로피는 로지스틱 손실함수의 기술적인 표현이다. 범주형 크로스 엔트로피는 소프트맥스를 사용하여 다중 클래스 예측으로 일반화한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_optimizer = tf.keras.optimizers.SGD(lr =0.001, decay = 1e-7, momentum = .9)\n",
    "model.compile(optimizer = sgd_optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 컴파일 한 후 fit 메서드를 호출하여 훈련시킨다. 여기서 미니 배치 경사 하강법을 사용한다. 배치마다 담긴 훈련 샘플 개수는 64개다. 50번의 에포크 동안 MLP를 훈련시킨다. verbose를 1로 설정하여 훈련하는 동안 비용함수의 최적화 과정을 따라간다.\n",
    "\n",
    "validation_split 매개변수는 유용하다. 0.1로 설정하면 훈련 데이터의 10퍼센트를 검정 데이터로 떼어낸다. 에포크마다 이 데이터로 검증 점수를 계산하므로 모델이 과대적합 되었는지 모니터링 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "54000/54000 [==============================] - 2s 42us/sample - loss: 0.7114 - val_loss: 0.3624\n",
      "Epoch 2/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.3711 - val_loss: 0.2736\n",
      "Epoch 3/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.3046 - val_loss: 0.2365\n",
      "Epoch 4/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.2677 - val_loss: 0.2128\n",
      "Epoch 5/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.2425 - val_loss: 0.1967\n",
      "Epoch 6/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.2232 - val_loss: 0.1842\n",
      "Epoch 7/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.2075 - val_loss: 0.1746\n",
      "Epoch 8/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.1942 - val_loss: 0.1662\n",
      "Epoch 9/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.1828 - val_loss: 0.1598\n",
      "Epoch 10/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.1728 - val_loss: 0.1543\n",
      "Epoch 11/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.1638 - val_loss: 0.1475\n",
      "Epoch 12/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.1556 - val_loss: 0.1430\n",
      "Epoch 13/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.1484 - val_loss: 0.1386\n",
      "Epoch 14/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.1414 - val_loss: 0.1360\n",
      "Epoch 15/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.1354 - val_loss: 0.1326\n",
      "Epoch 16/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.1298 - val_loss: 0.1278\n",
      "Epoch 17/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.1245 - val_loss: 0.1255\n",
      "Epoch 18/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.1197 - val_loss: 0.1226\n",
      "Epoch 19/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.1152 - val_loss: 0.1211\n",
      "Epoch 20/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.1109 - val_loss: 0.1184\n",
      "Epoch 21/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.1070 - val_loss: 0.1175\n",
      "Epoch 22/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.1033 - val_loss: 0.1150\n",
      "Epoch 23/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.0998 - val_loss: 0.1129\n",
      "Epoch 24/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.0963 - val_loss: 0.1122\n",
      "Epoch 25/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.0932 - val_loss: 0.1102\n",
      "Epoch 26/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.0904 - val_loss: 0.1098\n",
      "Epoch 27/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.0874 - val_loss: 0.1079\n",
      "Epoch 28/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.0847 - val_loss: 0.1080\n",
      "Epoch 29/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.0819 - val_loss: 0.1073\n",
      "Epoch 30/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.0796 - val_loss: 0.1057\n",
      "Epoch 31/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.0772 - val_loss: 0.1054\n",
      "Epoch 32/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.0749 - val_loss: 0.1049\n",
      "Epoch 33/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.0727 - val_loss: 0.1047\n",
      "Epoch 34/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.0706 - val_loss: 0.1046\n",
      "Epoch 35/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.0684 - val_loss: 0.1032\n",
      "Epoch 36/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.0665 - val_loss: 0.1027\n",
      "Epoch 37/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.0647 - val_loss: 0.1036\n",
      "Epoch 38/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.0628 - val_loss: 0.1032\n",
      "Epoch 39/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.0611 - val_loss: 0.1027\n",
      "Epoch 40/50\n",
      "54000/54000 [==============================] - 2s 28us/sample - loss: 0.0593 - val_loss: 0.1018\n",
      "Epoch 41/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.0578 - val_loss: 0.1019\n",
      "Epoch 42/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.0560 - val_loss: 0.1028\n",
      "Epoch 43/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.0547 - val_loss: 0.1012\n",
      "Epoch 44/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.0530 - val_loss: 0.1017\n",
      "Epoch 45/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.0516 - val_loss: 0.1017\n",
      "Epoch 46/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.0502 - val_loss: 0.1018\n",
      "Epoch 47/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.0490 - val_loss: 0.1012\n",
      "Epoch 48/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.0474 - val_loss: 0.1012\n",
      "Epoch 49/50\n",
      "54000/54000 [==============================] - 1s 26us/sample - loss: 0.0463 - val_loss: 0.1014\n",
      "Epoch 50/50\n",
      "54000/54000 [==============================] - 1s 27us/sample - loss: 0.0451 - val_loss: 0.1013\n"
     ]
    }
   ],
   "source": [
    "history  = model.fit(X_train_centered, Y_train_onehot,\n",
    "                    batch_size = 64, epochs = 50,\n",
    "                    verbose = 1,\n",
    "                    validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련하는 동안 비용 함수 값을 출력하는 기능은 유용하다. 훈련 도중에 비용이 감소하는 지 여부를 빨리 확인해서 감소 하지 않는다면 하이퍼 파라미터를 튜닝하기 위해 알고리즘을 일찍 멈출 수 있다. 클래스 레이블을 예측하려면 predict_calsses 메서드를 사용하여 정수로 된 클래스 레이블을 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 정확도: 98.96%\n"
     ]
    }
   ],
   "source": [
    "Y_train_pred = model.predict_classes(X_train_centered, verbose =0)\n",
    "correct_preds = np.sum(Y_train == Y_train_pred, axis = 0)\n",
    "train_acc = correct_preds / Y_train.shape[0]\n",
    "\n",
    "print('훈련 정확도: %.2f%%' % (train_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 정확도: 98.96%\n"
     ]
    }
   ],
   "source": [
    "Y_test_pred = model.predict_classes(X_test_centered, verbose = 0)\n",
    "correct_preds = np.sum(Y_test == Y_test_pred, axis = 0)\n",
    "test_acc = correct_preds / Y_test.shape[0]\n",
    "\n",
    "print('훈련 정확도: %.2f%%' % (train_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다층 신경망의 활성화 함수 선택\n",
    "\n",
    "기술적으로 미분 가능하다면 어떤 함수라도 다층 신경망의 활성화 함수로 사용할 수 있다,\n",
    "실제로는 은닉층이나 출력층에 선형 활성화 함수를 사용하는 것이 유용하지 않다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
